{
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.8",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "name": "Predicting authorized_flag in transactions_known",
    "hide_input": false,
    "modifiedBy": "admin"
  },
  "nbformat": 4,
  "nbformat_minor": 1,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Predicting authorized_flag in transactions_known"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Notebook automatically generated from your model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Model Random forest, trained on 2021-06-30 14:56:20."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Generated on 2022-06-13 10:15:07.332261"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "prediction\nThis notebook will reproduce the steps for a BINARY_CLASSIFICATION on  transactions_known.\nThe main objective is to predict the variable authorized_flag"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Warning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The goal of this notebook is to provide an easily readable and explainable code that reproduces the main steps\nof training the model. It is not complete: some of the preprocessing done by the DSS visual machine learning is not\nreplicated in this notebook. This notebook will not give the same results and model performance as the DSS visual machine\nlearning model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let\u0027s start with importing the required libs :"
      ]
    },
    {
      "execution_count": 1,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import sys\nimport dataiku\nimport numpy as np\nimport pandas as pd\nimport sklearn as sk\nimport dataiku.core.pandasutils as pdu\nfrom dataiku.doctor.preprocessing import PCA\nfrom collections import defaultdict, Counter"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "And tune pandas display options:"
      ]
    },
    {
      "execution_count": 2,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "pd.set_option(\u0027display.width\u0027, 3000)\npd.set_option(\u0027display.max_rows\u0027, 200)\npd.set_option(\u0027display.max_columns\u0027, 200)"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Importing base data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The first step is to get our machine learning dataset:"
      ]
    },
    {
      "execution_count": 3,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# We apply the preparation that you defined. You should not modify this.\npreparation_steps \u003d []\npreparation_output_schema \u003d {\u0027columns\u0027: [{\u0027name\u0027: \u0027transaction_id\u0027, \u0027type\u0027: \u0027bigint\u0027}, {\u0027name\u0027: \u0027authorized_flag\u0027, \u0027type\u0027: \u0027bigint\u0027}, {\u0027name\u0027: \u0027purchase_date\u0027, \u0027type\u0027: \u0027string\u0027}, {\u0027name\u0027: \u0027purchase_date_parsed\u0027, \u0027type\u0027: \u0027date\u0027}, {\u0027name\u0027: \u0027purchase_year\u0027, \u0027type\u0027: \u0027bigint\u0027}, {\u0027name\u0027: \u0027purchase_month\u0027, \u0027type\u0027: \u0027bigint\u0027}, {\u0027name\u0027: \u0027purchase_day\u0027, \u0027type\u0027: \u0027bigint\u0027}, {\u0027name\u0027: \u0027purchase_dow\u0027, \u0027type\u0027: \u0027bigint\u0027}, {\u0027name\u0027: \u0027purchase_weekend\u0027, \u0027type\u0027: \u0027bigint\u0027}, {\u0027name\u0027: \u0027purchase_hour\u0027, \u0027type\u0027: \u0027bigint\u0027}, {\u0027name\u0027: \u0027card_id\u0027, \u0027type\u0027: \u0027string\u0027}, {\u0027name\u0027: \u0027merchant_id\u0027, \u0027type\u0027: \u0027string\u0027}, {\u0027name\u0027: \u0027merchant_category_id\u0027, \u0027type\u0027: \u0027bigint\u0027}, {\u0027name\u0027: \u0027item_category\u0027, \u0027type\u0027: \u0027string\u0027}, {\u0027name\u0027: \u0027purchase_amount\u0027, \u0027type\u0027: \u0027double\u0027}, {\u0027name\u0027: \u0027signature_provided\u0027, \u0027type\u0027: \u0027bigint\u0027}, {\u0027name\u0027: \u0027card_first_active_month\u0027, \u0027type\u0027: \u0027string\u0027}, {\u0027name\u0027: \u0027card_first_active_month_parsed\u0027, \u0027type\u0027: \u0027date\u0027}, {\u0027name\u0027: \u0027days_active\u0027, \u0027type\u0027: \u0027bigint\u0027}, {\u0027name\u0027: \u0027card_reward_program\u0027, \u0027type\u0027: \u0027string\u0027}, {\u0027name\u0027: \u0027card_latitude\u0027, \u0027type\u0027: \u0027double\u0027}, {\u0027name\u0027: \u0027card_longitude\u0027, \u0027type\u0027: \u0027double\u0027}, {\u0027name\u0027: \u0027card_fico_score\u0027, \u0027type\u0027: \u0027bigint\u0027}, {\u0027name\u0027: \u0027card_age\u0027, \u0027type\u0027: \u0027bigint\u0027}, {\u0027name\u0027: \u0027merchant_subsector_description\u0027, \u0027type\u0027: \u0027string\u0027}, {\u0027name\u0027: \u0027merchant_latitude\u0027, \u0027type\u0027: \u0027double\u0027}, {\u0027name\u0027: \u0027merchant_longitude\u0027, \u0027type\u0027: \u0027double\u0027}, {\u0027name\u0027: \u0027merchant_location\u0027, \u0027type\u0027: \u0027string\u0027}, {\u0027name\u0027: \u0027merchant_state\u0027, \u0027type\u0027: \u0027string\u0027}, {\u0027name\u0027: \u0027merchant_state_enName\u0027, \u0027type\u0027: \u0027string\u0027}, {\u0027name\u0027: \u0027card_location\u0027, \u0027type\u0027: \u0027string\u0027}, {\u0027name\u0027: \u0027merchant_cardholder_distance\u0027, \u0027type\u0027: \u0027double\u0027}, {\u0027name\u0027: \u0027card_state\u0027, \u0027type\u0027: \u0027string\u0027}, {\u0027name\u0027: \u0027card_state_enName\u0027, \u0027type\u0027: \u0027string\u0027}], \u0027userModified\u0027: False}\n\nml_dataset_handle \u003d dataiku.Dataset(\u0027transactions_known\u0027)\nml_dataset_handle.set_preparation_steps(preparation_steps, preparation_output_schema)\n%time ml_dataset \u003d ml_dataset_handle.get_dataframe(limit \u003d 100000)\n\nprint (\u0027Base data has %i rows and %i columns\u0027 % (ml_dataset.shape[0], ml_dataset.shape[1]))\n# Five first records\",\nml_dataset.head(5)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "text": "CPU times: user 978 ms, sys: 90.5 ms, total: 1.07 s\nWall time: 3.77 s\nBase data has 100000 rows and 34 columns\n",
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "execution_count": 3,
          "data": {
            "text/plain": "   transaction_id  authorized_flag        purchase_date purchase_date_parsed  purchase_year  purchase_month  purchase_day  purchase_dow  purchase_weekend  purchase_hour          card_id      merchant_id  merchant_category_id item_category  purchase_amount  signature_provided card_first_active_month card_first_active_month_parsed  days_active   card_reward_program  card_latitude  card_longitude  card_fico_score  card_age merchant_subsector_description  merchant_latitude  merchant_longitude      merchant_location merchant_state merchant_state_enName          card_location  merchant_cardholder_distance     card_state card_state_enName\n0           73553                1  2017-06-03 08:56:44  2017-06-03 08:56:44           2017               6             3             6                 1              8  C_ID_b72230e635  M_ID_120024bfc9                   276             B           195.99                   0                 2016-12                     2016-12-01          184             cash_back         41.669         -73.920              636        97           consumer electronics             41.705             -73.951  POINT(-73.951 41.705)       New York              New York   POINT(-73.92 41.669)                          4.76       New York          New York\n1           81534                1  2017-06-15 20:58:47  2017-06-15 20:58:47           2017               6            15             4                 0             20  C_ID_b72230e635  M_ID_13cb138513                   198             A          1191.91                   0                 2016-12                     2016-12-01          196             cash_back         41.669         -73.920              636        97                      insurance             42.668             -73.808  POINT(-73.808 42.668)       New York              New York   POINT(-73.92 41.669)                        111.47       New York          New York\n2           32505                1  2017-03-15 16:55:33  2017-03-15 16:55:33           2017               3            15             3                 0             16  C_ID_b7224e1512  M_ID_2637773dd2                   422             B           266.60                   0                 2017-02                     2017-02-01           42  dining_entertainment         43.211         -71.577              544        28           consumer electronics             43.226             -71.544  POINT(-71.544 43.226)  New Hampshire         New Hampshire  POINT(-71.577 43.211)                          3.15  New Hampshire     New Hampshire\n3           64039                1  2017-05-16 18:25:50  2017-05-16 18:25:50           2017               5            16             2                 0             18  C_ID_b7224e1512  M_ID_f2ae17fdd2                   884             A            84.13                   0                 2017-02                     2017-02-01          104  dining_entertainment         43.211         -71.577              544        28           consumer electronics             41.743             -73.938  POINT(-73.938 41.743)       New York              New York  POINT(-71.577 43.211)                        253.24  New Hampshire     New Hampshire\n4           57791                1  2017-05-05 03:13:51  2017-05-05 03:13:51           2017               5             5             5                 0              3  C_ID_b72498b35a  M_ID_e5374dabc0                   130             D             8.72                   0                 2017-04                     2017-04-01           34  dining_entertainment         41.710         -73.974              665        25                   luxury goods             43.218             -71.526  POINT(-71.526 43.218)  New Hampshire         New Hampshire   POINT(-73.974 41.71)                        261.59       New York          New York",
            "text/html": "\n            \u003cbutton style\u003d\"display:none\" \n            class\u003d\"btn btn-default ipython-export-btn\" \n            id\u003d\"btn-df-35b5c0b8-816e-4d27-98b1-6ac4dfd15cc1\" \n            onclick\u003d\"_export_df(\u002735b5c0b8-816e-4d27-98b1-6ac4dfd15cc1\u0027)\"\u003e\n                Export dataframe\n            \u003c/button\u003e\n            \n            \u003cscript\u003e\n                \n                function _check_export_df_possible(dfid,yes_fn,no_fn) {\n                    console.log(\u0027Checking dataframe exportability...\u0027)\n                    if(!IPython || !IPython.notebook || !IPython.notebook.kernel || !IPython.notebook.kernel) {\n                        console.log(\u0027Export is not possible (IPython kernel is not available)\u0027)\n                        if(no_fn) {\n                            no_fn();\n                        }\n                    } else {\n                        var pythonCode \u003d \u0027from dataiku.notebook.export import IPythonExporter;IPythonExporter._check_export_stdout(\"\u0027+dfid+\u0027\")\u0027;\n                        IPython.notebook.kernel.execute(pythonCode,{iopub: {output: function(resp) {\n                            console.info(\"Exportability response\", resp);\n                            var size \u003d /^([0-9]+)x([0-9]+)$/.exec(resp.content.data || resp.content.text)\n                            if(!size) {\n                                console.log(\u0027Export is not possible (dataframe is not in-memory anymore)\u0027)\n                                if(no_fn) {\n                                    no_fn();\n                                }\n                            } else {\n                                console.log(\u0027Export is possible\u0027)\n                                if(yes_fn) {\n                                    yes_fn(1*size[1],1*size[2]);\n                                }\n                            }\n                        }}});\n                    }\n                }\n            \n                function _export_df(dfid) {\n                    \n                    var btn \u003d $(\u0027#btn-df-\u0027+dfid);\n                    var btns \u003d $(\u0027.ipython-export-btn\u0027);\n                    \n                    _check_export_df_possible(dfid,function() {\n                        \n                        window.parent.openExportModalFromIPython(\u0027Pandas dataframe\u0027,function(data) {\n                            btns.prop(\u0027disabled\u0027,true);\n                            btn.text(\u0027Exporting...\u0027);\n                            var command \u003d \u0027from dataiku.notebook.export import IPythonExporter;IPythonExporter._run_export(\"\u0027+dfid+\u0027\",\"\u0027+data.exportId+\u0027\")\u0027;\n                            var callback \u003d {iopub:{output: function(resp) {\n                                console.info(\"CB resp:\", resp);\n                                _check_export_df_possible(dfid,function(rows, cols) {\n                                    $(\u0027#btn-df-\u0027+dfid)\n                                        .css(\u0027display\u0027,\u0027inline-block\u0027)\n                                        .text(\u0027Export this dataframe (\u0027+rows+\u0027 rows, \u0027+cols+\u0027 cols)\u0027)\n                                        .prop(\u0027disabled\u0027,false);\n                                },function() {\n                                    $(\u0027#btn-df-\u0027+dfid).css(\u0027display\u0027,\u0027none\u0027);\n                                });\n                            }}};\n                            IPython.notebook.kernel.execute(command,callback,{silent:false}); // yes, silent now defaults to true. figures.\n                        });\n                    \n                    }, function(){\n                            alert(\u0027Unable to export : the Dataframe object is not loaded in memory\u0027);\n                            btn.css(\u0027display\u0027,\u0027none\u0027);\n                    });\n                    \n                }\n                \n                (function(dfid) {\n                \n                    var retryCount \u003d 10;\n                \n                    function is_valid_websock(s) {\n                        return s \u0026\u0026 s.readyState\u003d\u003d1;\n                    }\n                \n                    function check_conn() {\n                        \n                        if(!IPython || !IPython.notebook) {\n                            // Don\u0027t even try to go further\n                            return;\n                        }\n                        \n                        // Check if IPython is ready\n                        console.info(\"Checking conn ...\")\n                        if(IPython.notebook.kernel\n                        \u0026\u0026 IPython.notebook.kernel\n                        \u0026\u0026 is_valid_websock(IPython.notebook.kernel.ws)\n                        ) {\n                            \n                            _check_export_df_possible(dfid,function(rows, cols) {\n                                $(\u0027#btn-df-\u0027+dfid).css(\u0027display\u0027,\u0027inline-block\u0027);\n                                $(\u0027#btn-df-\u0027+dfid).text(\u0027Export this dataframe (\u0027+rows+\u0027 rows, \u0027+cols+\u0027 cols)\u0027);\n                            });\n                            \n                        } else {\n                            console.info(\"Conditions are not ok\", IPython.notebook.kernel);\n                            \n                            // Retry later\n                            \n                            if(retryCount\u003e0) {\n                                setTimeout(check_conn,500);\n                                retryCount--;\n                            }\n                            \n                        }\n                    };\n                    \n                    setTimeout(check_conn,100);\n                    \n                })(\"35b5c0b8-816e-4d27-98b1-6ac4dfd15cc1\");\n                \n            \u003c/script\u003e\n            \n        \u003cdiv\u003e\n\u003cstyle scoped\u003e\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n\u003c/style\u003e\n\u003ctable border\u003d\"1\" class\u003d\"dataframe\"\u003e\n  \u003cthead\u003e\n    \u003ctr style\u003d\"text-align: right;\"\u003e\n      \u003cth\u003e\u003c/th\u003e\n      \u003cth\u003etransaction_id\u003c/th\u003e\n      \u003cth\u003eauthorized_flag\u003c/th\u003e\n      \u003cth\u003epurchase_date\u003c/th\u003e\n      \u003cth\u003epurchase_date_parsed\u003c/th\u003e\n      \u003cth\u003epurchase_year\u003c/th\u003e\n      \u003cth\u003epurchase_month\u003c/th\u003e\n      \u003cth\u003epurchase_day\u003c/th\u003e\n      \u003cth\u003epurchase_dow\u003c/th\u003e\n      \u003cth\u003epurchase_weekend\u003c/th\u003e\n      \u003cth\u003epurchase_hour\u003c/th\u003e\n      \u003cth\u003ecard_id\u003c/th\u003e\n      \u003cth\u003emerchant_id\u003c/th\u003e\n      \u003cth\u003emerchant_category_id\u003c/th\u003e\n      \u003cth\u003eitem_category\u003c/th\u003e\n      \u003cth\u003epurchase_amount\u003c/th\u003e\n      \u003cth\u003esignature_provided\u003c/th\u003e\n      \u003cth\u003ecard_first_active_month\u003c/th\u003e\n      \u003cth\u003ecard_first_active_month_parsed\u003c/th\u003e\n      \u003cth\u003edays_active\u003c/th\u003e\n      \u003cth\u003ecard_reward_program\u003c/th\u003e\n      \u003cth\u003ecard_latitude\u003c/th\u003e\n      \u003cth\u003ecard_longitude\u003c/th\u003e\n      \u003cth\u003ecard_fico_score\u003c/th\u003e\n      \u003cth\u003ecard_age\u003c/th\u003e\n      \u003cth\u003emerchant_subsector_description\u003c/th\u003e\n      \u003cth\u003emerchant_latitude\u003c/th\u003e\n      \u003cth\u003emerchant_longitude\u003c/th\u003e\n      \u003cth\u003emerchant_location\u003c/th\u003e\n      \u003cth\u003emerchant_state\u003c/th\u003e\n      \u003cth\u003emerchant_state_enName\u003c/th\u003e\n      \u003cth\u003ecard_location\u003c/th\u003e\n      \u003cth\u003emerchant_cardholder_distance\u003c/th\u003e\n      \u003cth\u003ecard_state\u003c/th\u003e\n      \u003cth\u003ecard_state_enName\u003c/th\u003e\n    \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n    \u003ctr\u003e\n      \u003cth\u003e0\u003c/th\u003e\n      \u003ctd\u003e73553\u003c/td\u003e\n      \u003ctd\u003e1\u003c/td\u003e\n      \u003ctd\u003e2017-06-03 08:56:44\u003c/td\u003e\n      \u003ctd\u003e2017-06-03 08:56:44\u003c/td\u003e\n      \u003ctd\u003e2017\u003c/td\u003e\n      \u003ctd\u003e6\u003c/td\u003e\n      \u003ctd\u003e3\u003c/td\u003e\n      \u003ctd\u003e6\u003c/td\u003e\n      \u003ctd\u003e1\u003c/td\u003e\n      \u003ctd\u003e8\u003c/td\u003e\n      \u003ctd\u003eC_ID_b72230e635\u003c/td\u003e\n      \u003ctd\u003eM_ID_120024bfc9\u003c/td\u003e\n      \u003ctd\u003e276\u003c/td\u003e\n      \u003ctd\u003eB\u003c/td\u003e\n      \u003ctd\u003e195.99\u003c/td\u003e\n      \u003ctd\u003e0\u003c/td\u003e\n      \u003ctd\u003e2016-12\u003c/td\u003e\n      \u003ctd\u003e2016-12-01\u003c/td\u003e\n      \u003ctd\u003e184\u003c/td\u003e\n      \u003ctd\u003ecash_back\u003c/td\u003e\n      \u003ctd\u003e41.669\u003c/td\u003e\n      \u003ctd\u003e-73.920\u003c/td\u003e\n      \u003ctd\u003e636\u003c/td\u003e\n      \u003ctd\u003e97\u003c/td\u003e\n      \u003ctd\u003econsumer electronics\u003c/td\u003e\n      \u003ctd\u003e41.705\u003c/td\u003e\n      \u003ctd\u003e-73.951\u003c/td\u003e\n      \u003ctd\u003ePOINT(-73.951 41.705)\u003c/td\u003e\n      \u003ctd\u003eNew York\u003c/td\u003e\n      \u003ctd\u003eNew York\u003c/td\u003e\n      \u003ctd\u003ePOINT(-73.92 41.669)\u003c/td\u003e\n      \u003ctd\u003e4.76\u003c/td\u003e\n      \u003ctd\u003eNew York\u003c/td\u003e\n      \u003ctd\u003eNew York\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003cth\u003e1\u003c/th\u003e\n      \u003ctd\u003e81534\u003c/td\u003e\n      \u003ctd\u003e1\u003c/td\u003e\n      \u003ctd\u003e2017-06-15 20:58:47\u003c/td\u003e\n      \u003ctd\u003e2017-06-15 20:58:47\u003c/td\u003e\n      \u003ctd\u003e2017\u003c/td\u003e\n      \u003ctd\u003e6\u003c/td\u003e\n      \u003ctd\u003e15\u003c/td\u003e\n      \u003ctd\u003e4\u003c/td\u003e\n      \u003ctd\u003e0\u003c/td\u003e\n      \u003ctd\u003e20\u003c/td\u003e\n      \u003ctd\u003eC_ID_b72230e635\u003c/td\u003e\n      \u003ctd\u003eM_ID_13cb138513\u003c/td\u003e\n      \u003ctd\u003e198\u003c/td\u003e\n      \u003ctd\u003eA\u003c/td\u003e\n      \u003ctd\u003e1191.91\u003c/td\u003e\n      \u003ctd\u003e0\u003c/td\u003e\n      \u003ctd\u003e2016-12\u003c/td\u003e\n      \u003ctd\u003e2016-12-01\u003c/td\u003e\n      \u003ctd\u003e196\u003c/td\u003e\n      \u003ctd\u003ecash_back\u003c/td\u003e\n      \u003ctd\u003e41.669\u003c/td\u003e\n      \u003ctd\u003e-73.920\u003c/td\u003e\n      \u003ctd\u003e636\u003c/td\u003e\n      \u003ctd\u003e97\u003c/td\u003e\n      \u003ctd\u003einsurance\u003c/td\u003e\n      \u003ctd\u003e42.668\u003c/td\u003e\n      \u003ctd\u003e-73.808\u003c/td\u003e\n      \u003ctd\u003ePOINT(-73.808 42.668)\u003c/td\u003e\n      \u003ctd\u003eNew York\u003c/td\u003e\n      \u003ctd\u003eNew York\u003c/td\u003e\n      \u003ctd\u003ePOINT(-73.92 41.669)\u003c/td\u003e\n      \u003ctd\u003e111.47\u003c/td\u003e\n      \u003ctd\u003eNew York\u003c/td\u003e\n      \u003ctd\u003eNew York\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003cth\u003e2\u003c/th\u003e\n      \u003ctd\u003e32505\u003c/td\u003e\n      \u003ctd\u003e1\u003c/td\u003e\n      \u003ctd\u003e2017-03-15 16:55:33\u003c/td\u003e\n      \u003ctd\u003e2017-03-15 16:55:33\u003c/td\u003e\n      \u003ctd\u003e2017\u003c/td\u003e\n      \u003ctd\u003e3\u003c/td\u003e\n      \u003ctd\u003e15\u003c/td\u003e\n      \u003ctd\u003e3\u003c/td\u003e\n      \u003ctd\u003e0\u003c/td\u003e\n      \u003ctd\u003e16\u003c/td\u003e\n      \u003ctd\u003eC_ID_b7224e1512\u003c/td\u003e\n      \u003ctd\u003eM_ID_2637773dd2\u003c/td\u003e\n      \u003ctd\u003e422\u003c/td\u003e\n      \u003ctd\u003eB\u003c/td\u003e\n      \u003ctd\u003e266.60\u003c/td\u003e\n      \u003ctd\u003e0\u003c/td\u003e\n      \u003ctd\u003e2017-02\u003c/td\u003e\n      \u003ctd\u003e2017-02-01\u003c/td\u003e\n      \u003ctd\u003e42\u003c/td\u003e\n      \u003ctd\u003edining_entertainment\u003c/td\u003e\n      \u003ctd\u003e43.211\u003c/td\u003e\n      \u003ctd\u003e-71.577\u003c/td\u003e\n      \u003ctd\u003e544\u003c/td\u003e\n      \u003ctd\u003e28\u003c/td\u003e\n      \u003ctd\u003econsumer electronics\u003c/td\u003e\n      \u003ctd\u003e43.226\u003c/td\u003e\n      \u003ctd\u003e-71.544\u003c/td\u003e\n      \u003ctd\u003ePOINT(-71.544 43.226)\u003c/td\u003e\n      \u003ctd\u003eNew Hampshire\u003c/td\u003e\n      \u003ctd\u003eNew Hampshire\u003c/td\u003e\n      \u003ctd\u003ePOINT(-71.577 43.211)\u003c/td\u003e\n      \u003ctd\u003e3.15\u003c/td\u003e\n      \u003ctd\u003eNew Hampshire\u003c/td\u003e\n      \u003ctd\u003eNew Hampshire\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003cth\u003e3\u003c/th\u003e\n      \u003ctd\u003e64039\u003c/td\u003e\n      \u003ctd\u003e1\u003c/td\u003e\n      \u003ctd\u003e2017-05-16 18:25:50\u003c/td\u003e\n      \u003ctd\u003e2017-05-16 18:25:50\u003c/td\u003e\n      \u003ctd\u003e2017\u003c/td\u003e\n      \u003ctd\u003e5\u003c/td\u003e\n      \u003ctd\u003e16\u003c/td\u003e\n      \u003ctd\u003e2\u003c/td\u003e\n      \u003ctd\u003e0\u003c/td\u003e\n      \u003ctd\u003e18\u003c/td\u003e\n      \u003ctd\u003eC_ID_b7224e1512\u003c/td\u003e\n      \u003ctd\u003eM_ID_f2ae17fdd2\u003c/td\u003e\n      \u003ctd\u003e884\u003c/td\u003e\n      \u003ctd\u003eA\u003c/td\u003e\n      \u003ctd\u003e84.13\u003c/td\u003e\n      \u003ctd\u003e0\u003c/td\u003e\n      \u003ctd\u003e2017-02\u003c/td\u003e\n      \u003ctd\u003e2017-02-01\u003c/td\u003e\n      \u003ctd\u003e104\u003c/td\u003e\n      \u003ctd\u003edining_entertainment\u003c/td\u003e\n      \u003ctd\u003e43.211\u003c/td\u003e\n      \u003ctd\u003e-71.577\u003c/td\u003e\n      \u003ctd\u003e544\u003c/td\u003e\n      \u003ctd\u003e28\u003c/td\u003e\n      \u003ctd\u003econsumer electronics\u003c/td\u003e\n      \u003ctd\u003e41.743\u003c/td\u003e\n      \u003ctd\u003e-73.938\u003c/td\u003e\n      \u003ctd\u003ePOINT(-73.938 41.743)\u003c/td\u003e\n      \u003ctd\u003eNew York\u003c/td\u003e\n      \u003ctd\u003eNew York\u003c/td\u003e\n      \u003ctd\u003ePOINT(-71.577 43.211)\u003c/td\u003e\n      \u003ctd\u003e253.24\u003c/td\u003e\n      \u003ctd\u003eNew Hampshire\u003c/td\u003e\n      \u003ctd\u003eNew Hampshire\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003cth\u003e4\u003c/th\u003e\n      \u003ctd\u003e57791\u003c/td\u003e\n      \u003ctd\u003e1\u003c/td\u003e\n      \u003ctd\u003e2017-05-05 03:13:51\u003c/td\u003e\n      \u003ctd\u003e2017-05-05 03:13:51\u003c/td\u003e\n      \u003ctd\u003e2017\u003c/td\u003e\n      \u003ctd\u003e5\u003c/td\u003e\n      \u003ctd\u003e5\u003c/td\u003e\n      \u003ctd\u003e5\u003c/td\u003e\n      \u003ctd\u003e0\u003c/td\u003e\n      \u003ctd\u003e3\u003c/td\u003e\n      \u003ctd\u003eC_ID_b72498b35a\u003c/td\u003e\n      \u003ctd\u003eM_ID_e5374dabc0\u003c/td\u003e\n      \u003ctd\u003e130\u003c/td\u003e\n      \u003ctd\u003eD\u003c/td\u003e\n      \u003ctd\u003e8.72\u003c/td\u003e\n      \u003ctd\u003e0\u003c/td\u003e\n      \u003ctd\u003e2017-04\u003c/td\u003e\n      \u003ctd\u003e2017-04-01\u003c/td\u003e\n      \u003ctd\u003e34\u003c/td\u003e\n      \u003ctd\u003edining_entertainment\u003c/td\u003e\n      \u003ctd\u003e41.710\u003c/td\u003e\n      \u003ctd\u003e-73.974\u003c/td\u003e\n      \u003ctd\u003e665\u003c/td\u003e\n      \u003ctd\u003e25\u003c/td\u003e\n      \u003ctd\u003eluxury goods\u003c/td\u003e\n      \u003ctd\u003e43.218\u003c/td\u003e\n      \u003ctd\u003e-71.526\u003c/td\u003e\n      \u003ctd\u003ePOINT(-71.526 43.218)\u003c/td\u003e\n      \u003ctd\u003eNew Hampshire\u003c/td\u003e\n      \u003ctd\u003eNew Hampshire\u003c/td\u003e\n      \u003ctd\u003ePOINT(-73.974 41.71)\u003c/td\u003e\n      \u003ctd\u003e261.59\u003c/td\u003e\n      \u003ctd\u003eNew York\u003c/td\u003e\n      \u003ctd\u003eNew York\u003c/td\u003e\n    \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\u003c/div\u003e"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Initial data management"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The preprocessing aims at making the dataset compatible with modeling.\nAt the end of this step, we will have a matrix of float numbers, with no missing values.\nWe\u0027ll use the features and the preprocessing steps defined in Models.\n\nLet\u0027s only keep selected features"
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "ml_dataset \u003d ml_dataset[[\u0027card_fico_score\u0027, \u0027signature_provided\u0027, \u0027card_age\u0027, \u0027merchant_subsector_description\u0027, \u0027purchase_amount\u0027, \u0027merchant_state\u0027, \u0027authorized_flag\u0027]]"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let\u0027s first coerce categorical columns into unicode, numerical features into floats."
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# astype(\u0027unicode\u0027) does not work as expected\n\ndef coerce_to_unicode(x):\n    if sys.version_info \u003c (3, 0):\n        if isinstance(x, str):\n            return unicode(x,\u0027utf-8\u0027)\n        else:\n            return unicode(x)\n    else:\n        return str(x)\n\n\ncategorical_features \u003d [\u0027signature_provided\u0027, \u0027merchant_subsector_description\u0027, \u0027merchant_state\u0027]\nnumerical_features \u003d [\u0027card_fico_score\u0027, \u0027card_age\u0027, \u0027purchase_amount\u0027]\ntext_features \u003d []\nfrom dataiku.doctor.utils import datetime_to_epoch\nfor feature in categorical_features:\n    ml_dataset[feature] \u003d ml_dataset[feature].apply(coerce_to_unicode)\nfor feature in text_features:\n    ml_dataset[feature] \u003d ml_dataset[feature].apply(coerce_to_unicode)\nfor feature in numerical_features:\n    if ml_dataset[feature].dtype \u003d\u003d np.dtype(\u0027M8[ns]\u0027) or (hasattr(ml_dataset[feature].dtype, \u0027base\u0027) and ml_dataset[feature].dtype.base \u003d\u003d np.dtype(\u0027M8[ns]\u0027)):\n        ml_dataset[feature] \u003d datetime_to_epoch(ml_dataset[feature])\n    else:\n        ml_dataset[feature] \u003d ml_dataset[feature].astype(\u0027double\u0027)"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We are now going to handle the target variable and store it in a new variable:"
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "target_map \u003d {\u00270\u0027: 0, \u00271\u0027: 1}\nml_dataset[\u0027__target__\u0027] \u003d ml_dataset[\u0027authorized_flag\u0027].map(str).map(target_map)\ndel ml_dataset[\u0027authorized_flag\u0027]\n\n\n# Remove rows for which the target is unknown.\nml_dataset \u003d ml_dataset[~ml_dataset[\u0027__target__\u0027].isnull()]\n\nml_dataset[\u0027__target__\u0027] \u003d ml_dataset[\u0027__target__\u0027].astype(np.int64)"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Cross-validation strategy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The dataset needs to be split into 2 new sets, one that will be used for training the model (train set)\nand another that will be used to test its generalization capability (test set)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Important note: your model used a more advanced cross-validation strategy.\nFor the purpose of this notebook, it has been simplified to a random split of\na single dataset"
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "train, test \u003d pdu.split_train_valid(ml_dataset, prop\u003d0.8)\nprint (\u0027Train data has %i rows and %i columns\u0027 % (train.shape[0], train.shape[1]))\nprint (\u0027Test data has %i rows and %i columns\u0027 % (test.shape[0], test.shape[1]))"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Features preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The first thing to do at the features level is to handle the missing values.\nLet\u0027s reuse the settings defined in the model"
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "drop_rows_when_missing \u003d []\nimpute_when_missing \u003d [{\u0027feature\u0027: \u0027card_fico_score\u0027, \u0027impute_with\u0027: \u0027MEAN\u0027}, {\u0027feature\u0027: \u0027signature_provided\u0027, \u0027impute_with\u0027: \u0027MODE\u0027}, {\u0027feature\u0027: \u0027card_age\u0027, \u0027impute_with\u0027: \u0027MEAN\u0027}, {\u0027feature\u0027: \u0027purchase_amount\u0027, \u0027impute_with\u0027: \u0027MEAN\u0027}]\n\n# Features for which we drop rows with missing values\"\nfor feature in drop_rows_when_missing:\n    train \u003d train[train[feature].notnull()]\n    test \u003d test[test[feature].notnull()]\n    print (\u0027Dropped missing records in %s\u0027 % feature)\n\n# Features for which we impute missing values\"\nfor feature in impute_when_missing:\n    if feature[\u0027impute_with\u0027] \u003d\u003d \u0027MEAN\u0027:\n        v \u003d train[feature[\u0027feature\u0027]].mean()\n    elif feature[\u0027impute_with\u0027] \u003d\u003d \u0027MEDIAN\u0027:\n        v \u003d train[feature[\u0027feature\u0027]].median()\n    elif feature[\u0027impute_with\u0027] \u003d\u003d \u0027CREATE_CATEGORY\u0027:\n        v \u003d \u0027NULL_CATEGORY\u0027\n    elif feature[\u0027impute_with\u0027] \u003d\u003d \u0027MODE\u0027:\n        v \u003d train[feature[\u0027feature\u0027]].value_counts().index[0]\n    elif feature[\u0027impute_with\u0027] \u003d\u003d \u0027CONSTANT\u0027:\n        v \u003d feature[\u0027value\u0027]\n    train[feature[\u0027feature\u0027]] \u003d train[feature[\u0027feature\u0027]].fillna(v)\n    test[feature[\u0027feature\u0027]] \u003d test[feature[\u0027feature\u0027]].fillna(v)\n    print (\u0027Imputed missing values in feature %s with value %s\u0027 % (feature[\u0027feature\u0027], coerce_to_unicode(v)))"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can now handle the categorical features (still using the settings defined in Models):"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let\u0027s dummy-encode the following features.\nA binary column is created for each of the 100 most frequent values."
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "LIMIT_DUMMIES \u003d 100\n\ncategorical_to_dummy_encode \u003d [\u0027signature_provided\u0027, \u0027merchant_subsector_description\u0027]\n\n# Only keep the top 100 values\ndef select_dummy_values(train, features):\n    dummy_values \u003d {}\n    for feature in categorical_to_dummy_encode:\n        values \u003d [\n            value\n            for (value, _) in Counter(train[feature]).most_common(LIMIT_DUMMIES)\n        ]\n        dummy_values[feature] \u003d values\n    return dummy_values\n\nDUMMY_VALUES \u003d select_dummy_values(train, categorical_to_dummy_encode)\n\ndef dummy_encode_dataframe(df):\n    for (feature, dummy_values) in DUMMY_VALUES.items():\n        for dummy_value in dummy_values:\n            dummy_name \u003d u\u0027%s_value_%s\u0027 % (feature, coerce_to_unicode(dummy_value))\n            df[dummy_name] \u003d (df[feature] \u003d\u003d dummy_value).astype(float)\n        del df[feature]\n        print (\u0027Dummy-encoded feature %s\u0027 % feature)\n\ndummy_encode_dataframe(train)\n\ndummy_encode_dataframe(test)"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let\u0027s impact code the following featuress."
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "categorical_to_impact_code \u003d [\u0027merchant_state\u0027]\n\n# impact coding\nfrom dataiku.notebook import ImpactCoding\n\nimpact_coding \u003d ImpactCoding(\u0027multiple\u0027, m\u003d10)  # tune m\ntrain \u003d pd.concat([train, impact_coding.fit_transform(train[categorical_to_impact_code], train[\u0027__target__\u0027])], axis\u003d1)\nfor feature in categorical_to_impact_code:\n    del train[feature]\n\ntest \u003d pd.concat([test, impact_coding.transform(test[categorical_to_impact_code])], axis\u003d1)\nfor feature in categorical_to_impact_code:\n    del test[feature]\n\nfor feature in categorical_to_impact_code:\n    print (\u0027Impact Coded feature %s \u0027 % feature)"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let\u0027s rescale numerical features"
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "rescale_features \u003d {\u0027card_fico_score\u0027: \u0027AVGSTD\u0027, \u0027card_age\u0027: \u0027AVGSTD\u0027, \u0027purchase_amount\u0027: \u0027AVGSTD\u0027}\nfor (feature_name, rescale_method) in rescale_features.items():\n    if rescale_method \u003d\u003d \u0027MINMAX\u0027:\n        _min \u003d train[feature_name].min()\n        _max \u003d train[feature_name].max()\n        scale \u003d _max - _min\n        shift \u003d _min\n    else:\n        shift \u003d train[feature_name].mean()\n        scale \u003d train[feature_name].std()\n    if scale \u003d\u003d 0.:\n        del train[feature_name]\n        del test[feature_name]\n        print (\u0027Feature %s was dropped because it has no variance\u0027 % feature_name)\n    else:\n        print (\u0027Rescaled %s\u0027 % feature_name)\n        train[feature_name] \u003d (train[feature_name] - shift).astype(np.float64) / scale\n        test[feature_name] \u003d (test[feature_name] - shift).astype(np.float64) / scale"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Modeling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Before actually creating our model, we need to split the datasets into their features and labels parts:"
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "X_train \u003d train.drop(\u0027__target__\u0027, axis\u003d1)\nX_test \u003d test.drop(\u0027__target__\u0027, axis\u003d1)\n\ny_train \u003d np.array(train[\u0027__target__\u0027])\ny_test \u003d np.array(test[\u0027__target__\u0027])"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we can finally create our model!"
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\nclf \u003d RandomForestClassifier(n_estimators\u003d225,\n    random_state\u003d1337,\n    max_depth\u003d18,\n    min_samples_leaf\u003d1,\n    verbose\u003d2)"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We set \"class_weight\" as the weighting strategy:"
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "clf.class_weight \u003d \"balanced\""
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "... And train the model"
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "%time clf.fit(X_train, y_train)"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Build up our result dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The model is now trained, we can apply it to our test set:"
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "%time _predictions \u003d clf.predict(X_test)\n%time _probas \u003d clf.predict_proba(X_test)\npredictions \u003d pd.Series(data\u003d_predictions, index\u003dX_test.index, name\u003d\u0027predicted_value\u0027)\ncols \u003d [\n    u\u0027probability_of_value_%s\u0027 % label\n    for (_, label) in sorted([(int(target_map[label]), label) for label in target_map])\n]\nprobabilities \u003d pd.DataFrame(data\u003d_probas, index\u003dX_test.index, columns\u003dcols)\n\n# Build scored dataset\nresults_test \u003d X_test.join(predictions, how\u003d\u0027left\u0027)\nresults_test \u003d results_test.join(probabilities, how\u003d\u0027left\u0027)\nresults_test \u003d results_test.join(test[\u0027__target__\u0027], how\u003d\u0027left\u0027)\nresults_test \u003d results_test.rename(columns\u003d {\u0027__target__\u0027: \u0027authorized_flag\u0027})"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let\u0027s have a look at feature importances"
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "feature_importances_data \u003d []\nfeatures \u003d X_train.columns\nfor feature_name, feature_importance in zip(features, clf.feature_importances_):\n    feature_importances_data.append({\n        \u0027feature\u0027: feature_name,\n        \u0027importance\u0027: feature_importance\n    })\n\n# Plot the results\npd.DataFrame(feature_importances_data)\\\n    .set_index(\u0027feature\u0027)\\\n    .sort_values(by\u003d\u0027importance\u0027)[-10::]\\\n    .plot(title\u003d\u0027Top 10 most important variables\u0027,\n          kind\u003d\u0027barh\u0027,\n          figsize\u003d(10, 6),\n          color\u003d\u0027#348ABD\u0027,\n          alpha\u003d0.6,\n          lw\u003d\u00271\u0027,\n          edgecolor\u003d\u0027#348ABD\u0027,\n          grid\u003dFalse,)"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You can measure the model\u0027s accuracy:"
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from dataiku.doctor.utils.metrics import mroc_auc_score\ny_test_ser \u003d pd.Series(y_test)\n \nprint (\u0027AUC value:\u0027, mroc_auc_score(y_test_ser, _probas))"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can also view the predictions directly.\nSince scikit-learn only predicts numericals, the labels have been mapped to 0,1,2 ...\nWe need to \u0027reverse\u0027 the mapping to display the initial labels."
      ]
    },
    {
      "execution_count": 0,
      "cell_type": "code",
      "metadata": {},
      "source": [
        "inv_map \u003d { target_map[label] : label for label in target_map}\npredictions.map(inv_map)"
      ],
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "That\u0027s it. It\u0027s now up to you to tune your preprocessing, your algo, and your analysis !\n"
      ]
    }
  ]
}